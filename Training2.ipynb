{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6235b7b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6235b7b4",
        "outputId": "d8557d91-504f-4c20-f414-ea2777c092df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583ddac6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "583ddac6",
        "outputId": "1dd64b4a-3866-4ace-c4f8-ec2bcd7b67a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctgan\n",
            "  Downloading ctgan-0.10.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.29 in /usr/local/lib/python3.10/dist-packages (from ctgan) (4.66.6)\n",
            "Collecting rdt>=1.11.0 (from ctgan)\n",
            "  Downloading rdt-1.13.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from ctgan) (2.2.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from ctgan) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from ctgan) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->ctgan) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->ctgan) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->ctgan) (2024.2)\n",
            "Collecting Faker>=17 (from rdt>=1.11.0->ctgan)\n",
            "  Downloading Faker-33.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from rdt>=1.11.0->ctgan) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from rdt>=1.11.0->ctgan) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->ctgan) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->ctgan) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.11.0->ctgan) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.11.0->ctgan) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->ctgan) (3.0.2)\n",
            "Downloading ctgan-0.10.2-py3-none-any.whl (23 kB)\n",
            "Downloading rdt-1.13.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Faker-33.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Faker, rdt, ctgan\n",
            "Successfully installed Faker-33.0.0 ctgan-0.10.2 rdt-1.13.1\n"
          ]
        }
      ],
      "source": [
        "pip install ctgan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9bc46f",
      "metadata": {
        "id": "fd9bc46f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(r'C:\\Users\\jerre\\Downloads\\Base.csv.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f7cfe3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f7cfe3a",
        "outputId": "d5928e01-cc17-472d-f629-2197d85c0a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Base.csv.zip'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "WJROQRKRXSrX"
      },
      "id": "WJROQRKRXSrX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/feedzai/bank-account-fraud.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWT5f28aH3G-",
        "outputId": "f94debc6-8aae-4349-8008-1c10eed0d774"
      },
      "id": "fWT5f28aH3G-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bank-account-fraud'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 110 (delta 54), reused 64 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (110/110), 1.13 MiB | 5.41 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d589f9",
      "metadata": {
        "id": "13d589f9"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import ctgan\n",
        "import logging\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import yaml\n",
        "\n",
        "from contextlib import redirect_stdout, redirect_stderr\n",
        "from dataclasses import dataclass, field\n",
        "from numpy import random\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "from typing import Dict, Union\n",
        "\n",
        "\n",
        "from random_search import RandomValueTrial, suggest_callable_hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f697af",
      "metadata": {
        "id": "e2f697af"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_FEATURES = [\n",
        "    'source',\n",
        "    'payment_type',\n",
        "    'device_os',\n",
        "    'housing_status',\n",
        "    'employment_status',\n",
        "    'month'\n",
        "]\n",
        "\n",
        "BOOLEAN_FEATURES = [\n",
        "    'email_is_free',\n",
        "    'fraud_bool',\n",
        "    'foreign_request',\n",
        "    'keep_alive_session',\n",
        "    'phone_home_valid',\n",
        "    'phone_mobile_valid',\n",
        "    'has_other_cards',\n",
        "]\n",
        "\n",
        "N_RUNS = 4\n",
        "\n",
        "TARGET_FPR = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837f7b70",
      "metadata": {
        "id": "837f7b70"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'run_dir': '<run_dir>',\n",
        "    'config_path': 'config-rs.yml',\n",
        "    'n_procs': 4,\n",
        "    'devices': [\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"],\n",
        "    'log_level': \"DEBUG\",\n",
        "    'seed': 42,\n",
        "    'dry_run': False,\n",
        "    'dev_run': False,\n",
        "    'n_trials': 100,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef946c5",
      "metadata": {
        "id": "aef946c5"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RunConfig:\n",
        "    \"\"\"Dataclass with required information to train a model.\"\"\"\n",
        "    model_id: int\n",
        "\n",
        "    train_df: pd.DataFrame = field(repr=False)\n",
        "    val_df: pd.DataFrame = field(repr=False)\n",
        "    discrete_columns: list\n",
        "\n",
        "    model_run_dir: str\n",
        "\n",
        "    config: Dict\n",
        "\n",
        "    seed: Union[int, None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c9f4a3",
      "metadata": {
        "id": "33c9f4a3"
      },
      "outputs": [],
      "source": [
        "def configure_logging(log_arg):\n",
        "    received_level = getattr(logging, log_arg.upper(), None)\n",
        "\n",
        "    logging_level = received_level if received_level else logging.INFO\n",
        "\n",
        "    logging.basicConfig(\n",
        "        format='[ %(levelname)s ] %(asctime)s (%(process)s-%(processName)s) %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',\n",
        "        level=logging_level\n",
        "    )\n",
        "\n",
        "    if not received_level:\n",
        "        logging.warning('Unknown logging level %s: Setting logging to INFO', log_arg.upper())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6471b29a",
      "metadata": {
        "id": "6471b29a"
      },
      "outputs": [],
      "source": [
        "def create_run_dir(run_dir):\n",
        "    if run_dir.exists():\n",
        "        logging.error('Run Directory already exists: \\'%s\\'', run_dir)\n",
        "        exit(1)\n",
        "\n",
        "    os.mkdir(run_dir)\n",
        "\n",
        "    logging.info('Run results stored at: \\'%s\\'', run_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e9af29",
      "metadata": {
        "id": "44e9af29"
      },
      "outputs": [],
      "source": [
        "def read_configurations(config_path):\n",
        "    logging.info('Reading configurations from %s', config_path)\n",
        "\n",
        "    with open(config_path, 'r') as f:\n",
        "        configs = yaml.safe_load(f)\n",
        "\n",
        "    return configs['data'], configs['sweep_params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0286a6",
      "metadata": {
        "id": "8e0286a6"
      },
      "outputs": [],
      "source": [
        "def load_data(data_config):\n",
        "\n",
        "    logging.info('Loading train dataset from \\'%s\\'', data_config['train'])\n",
        "    logging.info('Loading validation dataset from \\'%s\\'', data_config['validation'])\n",
        "\n",
        "    train_df = pd.read_csv(data_config['train'], index_col=0)\n",
        "    val_df = pd.read_csv(data_config['validation'], index_col=0)\n",
        "\n",
        "    if 'keep' in data_config:\n",
        "        train_df = train_df[data_config['keep']]\n",
        "        val_df = val_df[data_config['keep']]\n",
        "    elif 'remove' in data_config:\n",
        "        train_df = train_df.drop(columns=data_config['remove'])\n",
        "        val_df = val_df.drop(columns=data_config['remove'])\n",
        "\n",
        "    discrete_columns = [f for f in CATEGORICAL_FEATURES + BOOLEAN_FEATURES if f in train_df.columns]\n",
        "\n",
        "    logging.info('Train Dataset: %s Features, %s Rows', len(train_df.columns), len(train_df))\n",
        "    logging.info('Validation Dataset: %s Features, %s Rows', len(val_df.columns), len(val_df))\n",
        "    logging.debug('Train Features: %s', list(train_df.columns))\n",
        "    logging.debug('Validation features: %s', list(val_df.columns))\n",
        "    logging.debug('Discrete columns: %s', discrete_columns)\n",
        "\n",
        "    return train_df, val_df, discrete_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab6bff30",
      "metadata": {
        "id": "ab6bff30"
      },
      "outputs": [],
      "source": [
        "# Just path functions\n",
        "\n",
        "def pad_int(model_id, zfill=3):\n",
        "    return str(model_id).zfill(zfill)\n",
        "\n",
        "def model_run_dir(run_dir, model_id):\n",
        "    return run_dir / pad_int(model_id)\n",
        "\n",
        "def config_path(model_run_dir, model_id):\n",
        "    return model_run_dir / f'config-{pad_int(model_id)}.yml'\n",
        "\n",
        "\n",
        "def model_path(model_run_dir, model_id):\n",
        "    return model_run_dir / f'model-{pad_int(model_id)}.pkl'\n",
        "\n",
        "\n",
        "def train_dataset_path(model_run_dir, model_id):\n",
        "    return model_run_dir / f'train-dataset-{pad_int(model_id)}.csv'\n",
        "\n",
        "\n",
        "def synthetic_dataset_path(model_run_dir, model_id):\n",
        "    return model_run_dir / f'synthetic-dataset-{pad_int(model_id)}.csv'\n",
        "\n",
        "\n",
        "def model_evaluation_path(model_run_dir, model_id):\n",
        "    return model_run_dir / f'evaluation-{pad_int(model_id)}.csv'\n",
        "\n",
        "\n",
        "def stdout_path(model_run_dir, model_id):\n",
        "    return model_run_dir / f'stdout-{pad_int(model_id)}.log'\n",
        "\n",
        "\n",
        "def stderr_path(model_run_dir, model_id):\n",
        "    return model_run_dir/ f'stderr-{pad_int(model_id)}.log'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7866c4de",
      "metadata": {
        "id": "7866c4de"
      },
      "outputs": [],
      "source": [
        "def build_run_configs(\n",
        "        run_dir: str,\n",
        "        datasets_config: dict,\n",
        "        data_sweep_params: dict,\n",
        "        model_sweep_params: dict,\n",
        "        devices: list,\n",
        "        n_trials: int,\n",
        "        seed: int,\n",
        ") -> list:\n",
        "    train_df, val_df, discrete_columns = load_data(datasets_config)\n",
        "\n",
        "    run_configs = []\n",
        "\n",
        "    random.seed(seed)\n",
        "    seeds = random.randint(n_trials*1000, size=n_trials)\n",
        "    for i, seed  in enumerate(seeds, start=1):\n",
        "        # Method to random sample configurations\n",
        "        configs_data =  suggest_callable_hyperparams(RandomValueTrial(seed=seed), data_sweep_params)\n",
        "        configs_model = suggest_callable_hyperparams(RandomValueTrial(seed=seed), model_sweep_params['kwargs'])\n",
        "        configs_model['generator_dim'] = eval(configs_model['generator_dim'])\n",
        "        configs_model['discriminator_dim'] = eval(configs_model['discriminator_dim'])\n",
        "        configs_model['generator_decay'] = eval(configs_model['generator_decay'])\n",
        "        configs_model['discriminator_decay'] = eval(configs_model['discriminator_decay'])\n",
        "        configs_model['cuda'] = devices[i % len(devices)]\n",
        "        config = {\"data\": configs_data, \"model\": configs_model}\n",
        "        run_configs.append(\n",
        "            RunConfig(\n",
        "                model_id=i,\n",
        "                train_df=train_df,\n",
        "                val_df=val_df,\n",
        "                discrete_columns=discrete_columns,\n",
        "                model_run_dir=model_run_dir(run_dir, i),\n",
        "                config=config,\n",
        "                seed=seed,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    return run_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eacd50be",
      "metadata": {
        "id": "eacd50be"
      },
      "outputs": [],
      "source": [
        "def subsample_with_prevalence(df, prevalence, seed):\n",
        "    if prevalence:\n",
        "        fraud = df[df['fraud_bool'] == 1]\n",
        "        non_fraud = df[df['fraud_bool'] == 0]\n",
        "\n",
        "        fraud_proportion, non_fraud_proportion = prevalence\n",
        "        non_fraud_instances = (len(fraud) * non_fraud_proportion) // fraud_proportion\n",
        "        if non_fraud_instances >= len(non_fraud):\n",
        "            logging.warning(\n",
        "                'Unable to subsample dataframe: Expected more than %s negative examples but got %s',\n",
        "                non_fraud_instances,\n",
        "                len(fraud)\n",
        "            )\n",
        "            non_fraud_sample = non_fraud\n",
        "        else:\n",
        "            non_fraud_sample = non_fraud.sample(n=non_fraud_instances, random_state=seed)\n",
        "        return utils.shuffle(pd.concat((fraud, non_fraud_sample)), random_state=seed)\n",
        "    else:\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac67628c",
      "metadata": {
        "id": "ac67628c"
      },
      "outputs": [],
      "source": [
        "def apply_config_to_data(df, data_config, model_id, seed):\n",
        "\n",
        "    if 'prevalence' in data_config:\n",
        "        df = subsample_with_prevalence(df, eval(data_config['prevalence']), seed)\n",
        "\n",
        "    if logging.root.isEnabledFor(logging.DEBUG):\n",
        "        logging.debug(\n",
        "            'Model %s: Dataset with %s Examples (%s fraud, %s non fraud)',\n",
        "            pad_int(model_id),\n",
        "            len(df),\n",
        "            len(df[df['fraud_bool'] == 1]),\n",
        "            len(df[df['fraud_bool'] == 0])\n",
        "        )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958a0095",
      "metadata": {
        "id": "958a0095"
      },
      "outputs": [],
      "source": [
        "def preprocess_categorical(train_df, val_df, discrete_columns):\n",
        "\n",
        "    categorical_columns = np.intersect1d(discrete_columns, CATEGORICAL_FEATURES)\n",
        "\n",
        "    for column in categorical_columns:\n",
        "        train_unique = train_df[column].unique()\n",
        "        val_unique = val_df[column].unique()\n",
        "        nans = np.setdiff1d(val_unique, train_unique)\n",
        "        val_df.loc[val_df[column].isin(nans), [column]] = np.nan\n",
        "\n",
        "    train_dummy = pd.get_dummies(train_df, columns=categorical_columns, dummy_na=True)\n",
        "    val_dummy = pd.get_dummies(val_df, columns=categorical_columns, dummy_na=True)\n",
        "\n",
        "    for unseen_column in np.setdiff1d(train_dummy.columns, val_dummy.columns):\n",
        "        val_dummy[unseen_column] = 0\n",
        "\n",
        "    return train_dummy, val_dummy\n",
        "\n",
        "def split(train_df, val_df, target):\n",
        "    train_x = train_df.drop(columns=[target])\n",
        "    train_y = train_df[target]\n",
        "    val_x = val_df.drop(columns=[target])\n",
        "    val_y = val_df[target]\n",
        "    return train_x, train_y, val_x, val_y\n",
        "\n",
        "def preprocess_and_split(train_df, val_df, discrete_columns, target):\n",
        "    train_dummy_df, val_dummy_df = preprocess_categorical(train_df, val_df, discrete_columns)\n",
        "    return split(train_dummy_df, val_dummy_df, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d148445",
      "metadata": {
        "id": "2d148445"
      },
      "outputs": [],
      "source": [
        "def class_index(model, class_value):\n",
        "    return np.argwhere(model.classes_ == class_value)[0]\n",
        "\n",
        "\n",
        "def prediction_probabilities(model, x):\n",
        "    return model.predict_proba(x)[:, class_index(model, 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f22ef4a",
      "metadata": {
        "id": "3f22ef4a"
      },
      "outputs": [],
      "source": [
        "def ordinal_encode(train_df, val_df, categorical_features):\n",
        "    for f in categorical_features:\n",
        "        enc = preprocessing.OrdinalEncoder()\n",
        "        train_df[f] = enc.fit_transform(train_df[[f]])\n",
        "        val_df[f] = enc.fit_transform(val_df[[f]])\n",
        "    return train_df, val_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668c5717",
      "metadata": {
        "id": "668c5717"
      },
      "outputs": [],
      "source": [
        "def compile_results(\n",
        "        real_fprs,\n",
        "        real_tprs,\n",
        "        real_thresholds,\n",
        "        synthetic_train_fprs,\n",
        "        synthetic_train_tprs,\n",
        "        synthetic_train_thresholds,\n",
        "        synthetic_val_fprs,\n",
        "        synthetic_val_tprs,\n",
        "        synthetic_val_thresholds,\n",
        "        synthetic_both_fprs,\n",
        "        synthetic_both_tprs,\n",
        "        synthetic_both_thresholds):\n",
        "\n",
        "    records = []\n",
        "    for i, results in enumerate(zip(real_fprs, real_tprs, real_thresholds), start=1):\n",
        "        for fpr, tpr, threshold in zip(*results):\n",
        "            records.append((i, fpr, tpr, threshold, 'real'))\n",
        "\n",
        "    for j, results in enumerate(zip(synthetic_train_fprs, synthetic_train_tprs, synthetic_train_thresholds), start=i+1):\n",
        "        for fpr, tpr, threshold in zip(*results):\n",
        "            records.append((j, fpr, tpr, threshold, 'synthetic-train'))\n",
        "\n",
        "    for k, results in enumerate(zip(synthetic_val_fprs, synthetic_val_tprs, synthetic_val_thresholds), start=j+1):\n",
        "        for fpr, tpr, threshold in zip(*results):\n",
        "            records.append((k, fpr, tpr, threshold, 'synthetic-val'))\n",
        "\n",
        "    for n, results in enumerate(zip(synthetic_both_fprs, synthetic_both_tprs, synthetic_both_thresholds), start=k+1):\n",
        "        for fpr, tpr, threshold in zip(*results):\n",
        "            records.append((n, fpr, tpr, threshold, 'synthetic-both'))\n",
        "\n",
        "    return pd.DataFrame.from_records(records, columns=['run_id', 'fpr', 'tpr', 'threshold', 'discrimination'])\n",
        "\n",
        "\n",
        "def summarize_results(\n",
        "        real_fprs,\n",
        "        real_tprs,\n",
        "        real_thresholds,\n",
        "        synthetic_train_fprs,\n",
        "        synthetic_train_tprs,\n",
        "        synthetic_train_thresholds,\n",
        "        synthetic_val_fprs,\n",
        "        synthetic_val_tprs,\n",
        "        synthetic_val_thresholds,\n",
        "        synthetic_both_fprs,\n",
        "        synthetic_both_tprs,\n",
        "        synthetic_both_thresholds):\n",
        "\n",
        "    def compute_avg_tpr_and_threshold(fprs, tprs, thresholds):\n",
        "        avg_tpr = 0\n",
        "        avg_threshold = 0\n",
        "\n",
        "        for run_fprs, run_tprs, run_thresholds in zip(fprs, tprs, thresholds):\n",
        "            target_fpr_index = np.argwhere(run_fprs <= TARGET_FPR).ravel()[-1]\n",
        "            avg_tpr += run_tprs[target_fpr_index] / N_RUNS\n",
        "            avg_threshold += run_thresholds[target_fpr_index] / N_RUNS\n",
        "\n",
        "        return avg_tpr, avg_threshold\n",
        "\n",
        "    avg_real_tpr, avg_real_threshold = \\\n",
        "        compute_avg_tpr_and_threshold(real_fprs, real_tprs, real_thresholds)\n",
        "\n",
        "    avg_synthetic_train_tpr, avg_synthetic_train_threshold = \\\n",
        "        compute_avg_tpr_and_threshold(synthetic_train_fprs, synthetic_train_tprs, synthetic_train_thresholds)\n",
        "\n",
        "    avg_synthetic_val_tpr, avg_synthetic_val_threshold = \\\n",
        "        compute_avg_tpr_and_threshold(synthetic_val_fprs, synthetic_val_tprs, synthetic_val_thresholds)\n",
        "\n",
        "    avg_synthetic_both_tpr, avg_synthetic_both_threshold = \\\n",
        "        compute_avg_tpr_and_threshold(synthetic_both_fprs, synthetic_both_tprs, synthetic_both_thresholds)\n",
        "\n",
        "    return (\n",
        "        avg_real_tpr,\n",
        "        avg_real_threshold,\n",
        "        avg_synthetic_train_tpr,\n",
        "        avg_synthetic_train_threshold,\n",
        "        avg_synthetic_val_tpr,\n",
        "        avg_synthetic_val_threshold,\n",
        "        avg_synthetic_both_tpr,\n",
        "        avg_synthetic_both_threshold\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9420cc23",
      "metadata": {
        "id": "9420cc23"
      },
      "outputs": [],
      "source": [
        "def run_instance(run_config: RunConfig):\n",
        "\n",
        "    model_id = run_config.model_id\n",
        "\n",
        "    train_df = run_config.train_df\n",
        "    val_df = run_config.val_df\n",
        "    discrete_columns = run_config.discrete_columns\n",
        "\n",
        "    model_run_dir = pathlib.Path(run_config.model_run_dir)\n",
        "\n",
        "    config = run_config.config\n",
        "    config_save_path = config_path(model_run_dir, model_id)\n",
        "    model_save_path = model_path(model_run_dir, model_id)\n",
        "    model_evaluation_save_path = model_evaluation_path(model_run_dir, model_id)\n",
        "    train_data_save_path = train_dataset_path(model_run_dir, model_id)\n",
        "    synthetic_data_save_path = synthetic_dataset_path(model_run_dir, model_id)\n",
        "\n",
        "    run_stdout_path = stdout_path(model_run_dir, model_id)\n",
        "    run_stderr_path = stderr_path(model_run_dir, model_id)\n",
        "\n",
        "    seed = run_config.seed\n",
        "\n",
        "    logging.info('Model %s: Training started', pad_int(model_id))\n",
        "    logging.debug('Model %s: Config %s', pad_int(model_id), config)\n",
        "    logging.debug('Model %s: Saved config to \\'%s\\'', pad_int(model_id), config_save_path)\n",
        "    logging.debug('Model %s: Stdout redirected to \\'%s\\'', pad_int(model_id), run_stdout_path)\n",
        "    logging.debug('Model %s: Stderr redirected to \\'%s\\'', pad_int(model_id), run_stderr_path)\n",
        "\n",
        "    data_config = config['data']\n",
        "    model_config = config['model']\n",
        "\n",
        "\n",
        "    df = pd.concat((train_df, val_df))\n",
        "    df = utils.shuffle(df)\n",
        "\n",
        "    df = apply_config_to_data(df, data_config, model_id, seed)\n",
        "\n",
        "    discrete_columns = copy.copy(discrete_columns)\n",
        "\n",
        "\n",
        "    os.mkdir(model_run_dir)\n",
        "\n",
        "    df.to_csv(train_data_save_path)\n",
        "    logging.debug('Model %s: Training data saved to %s', pad_int(model_id), train_data_save_path)\n",
        "\n",
        "    with open(config_save_path, 'w') as fd:\n",
        "        yaml.safe_dump(config, stream=fd, default_flow_style=False)\n",
        "\n",
        "    model = ctgan.CTGANSynthesizer(**model_config)\n",
        "\n",
        "    with open(run_stdout_path, 'w') as out_fd, open(run_stderr_path, 'w') as err_fd:\n",
        "        with redirect_stdout(out_fd), redirect_stderr(err_fd):\n",
        "            model.fit(df, discrete_columns)\n",
        "\n",
        "    model.save(model_save_path)\n",
        "\n",
        "    logging.info('Model %s: Saved model to \\'%s\\'', pad_int(model_id), model_save_path)\n",
        "\n",
        "    synthetic_df = model.sample(len(df))\n",
        "    synthetic_df.to_csv(synthetic_data_save_path)\n",
        "\n",
        "    logging.info('Model %s: Saved synthetic dataset to \\'%s\\'', pad_int(model_id), synthetic_data_save_path)\n",
        "\n",
        "    return model_id, synthetic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a58b47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02a58b47",
        "outputId": "12c4aec5-e93c-4d15-d35c-6a1f8f2465ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Successfully\n"
          ]
        }
      ],
      "source": [
        "def create_run_dir(run_dir):\n",
        "    if not run_dir:\n",
        "        logging.error('Run directory not provided or invalid.')\n",
        "        exit(1)\n",
        "    if run_dir.exists():\n",
        "        logging.error('Run Directory already exists: \\'%s\\'', run_dir)\n",
        "        exit(1)\n",
        "    try:\n",
        "        os.mkdir(run_dir)\n",
        "    except Exception as e:\n",
        "        logging.error('Failed to create run directory: %s', e)\n",
        "        exit(1)\n",
        "\n",
        "def read_configurations(config_path):\n",
        "    # Mock function: Replace with your actual implementation\n",
        "    logging.info(\"Reading configurations from: %s\", config_path)\n",
        "    datasets_config = {}\n",
        "    sweep_params = {'data': {}, 'model': {}}\n",
        "    return datasets_config, sweep_params\n",
        "\n",
        "def build_run_configs(run_dir, datasets_config, data_sweep_params, model_sweep_params, devices, n_trials, seed):\n",
        "    # Mock function: Replace with your actual implementation\n",
        "    logging.info(\"Building run configurations.\")\n",
        "    return [{}] * n_trials  # Simulate a list of configurations\n",
        "\n",
        "def run_instance(config):\n",
        "    # Mock function: Replace with your actual implementation\n",
        "    logging.info(\"Running instance with config: %s\", config)\n",
        "    return None  # Replace with the actual synthetic data if applicable\n",
        "\n",
        "def configure_logging(log_level):\n",
        "    logging.basicConfig(level=getattr(logging, log_level.upper(), 'INFO'),\n",
        "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def run_experiment():\n",
        "    args = {\n",
        "        'run_dir': '/run_dir',  # Replace with your actual directory\n",
        "        'config_path': 'config.yaml',            # Replace with your config file path\n",
        "        'n_procs': 4,\n",
        "        'devices': ['cpu'],\n",
        "        'dry_run': False,\n",
        "        'seed': 42,\n",
        "        'n_trials': 10,\n",
        "        'log_level': 'INFO'\n",
        "    }\n",
        "\n",
        "    run_dir = pathlib.Path(args['run_dir'])\n",
        "    config_path = args['config_path']\n",
        "    n_procs = args['n_procs']\n",
        "    devices = args['devices']\n",
        "    dry_run = args['dry_run']\n",
        "    seed = args['seed']\n",
        "    n_trials = args['n_trials']\n",
        "\n",
        "    configure_logging(args['log_level'])\n",
        "\n",
        "    if seed:\n",
        "        seed = int(seed)\n",
        "        logging.info('Using seed value: %s', seed)\n",
        "\n",
        "    if not run_dir.is_absolute():\n",
        "        logging.error(\"Invalid path for run directory: %s\", run_dir)\n",
        "        exit(1)\n",
        "\n",
        "    create_run_dir(run_dir)\n",
        "\n",
        "    datasets_config, sweep_params = read_configurations(config_path)\n",
        "\n",
        "    data_sweep_params = sweep_params['data']\n",
        "    model_sweep_params = sweep_params['model']\n",
        "\n",
        "    run_configs = build_run_configs(\n",
        "        run_dir,\n",
        "        datasets_config,\n",
        "        data_sweep_params,\n",
        "        model_sweep_params,\n",
        "        devices,\n",
        "        n_trials,\n",
        "        seed\n",
        "    )\n",
        "\n",
        "    dfs = []\n",
        "    with mp.Pool(n_procs) as pool:\n",
        "        for synthetic_df in pool.imap_unordered(run_instance, run_configs):\n",
        "            if not dry_run:\n",
        "                dfs.append(synthetic_df)\n",
        "\n",
        "    logging.info('Finished Successfully')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_experiment()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f65c233",
      "metadata": {
        "id": "5f65c233"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}